2022-06-06 09:17:48 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17088
2022-06-06 09:17:48 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17088
2022-06-06 09:17:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-06 09:17:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-06 09:17:48 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-06-06 09:17:48 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
2022-06-06 09:17:48 | INFO | fairseq.distributed.utils | initialized host IR-86A4D2-FCC3 as rank 0
2022-06-06 09:17:48 | INFO | fairseq.distributed.utils | initialized host IR-86A4D2-FCC3 as rank 1
[2022-06-06 09:17:56,160][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'wav2vec2_icbhi', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17088', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 16, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 16, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2000, 'max_update': 4000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'icbhi', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_seq2seq', 'w2v_path': '/media/SSD/tungtk2/fairseq/outputs/2022-05-28/07-32-46/checkpoints/checkpoint_best.pt', 'no_pretrained_weights': True, 'dropout_input': 0.0, 'final_dropout': 0.5, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.5, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 32, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 1.0, 'layerdrop': 0.5, 'mask_channel_min_space': 1, 'mask_channel_before': False, 'normalize': False, 'data': '/media/SSD/tungtk2/fairseq/data//ICBHI_256_32_unnormalized_log_augmented', 'w2v_args': None, 'offload_activations': False, 'min_params_to_wrap': 100000000, 'checkpoint_activations': False, 'ddp_backend': 'legacy_ddp', 'clf_hidden_dim': 256, 'clf_dropout_rate': 0.5, 'clf_output_dim': 4, 'decoder_embed_dim': 384, 'decoder_ffn_embed_dim': 3072, 'decoder_layers': 6, 'decoder_layerdrop': 0.0, 'decoder_attention_heads': 4, 'decoder_learned_pos': False, 'decoder_normalize_before': False, 'no_token_positional_embeddings': False, 'decoder_dropout': 0.0, 'decoder_attention_dropout': 0.0, 'decoder_activation_dropout': 0.0, 'max_target_positions': 2048, 'share_decoder_input_output_embed': False, 'autoregressive': False, 'num_classes': 2}, 'task': {'_name': 'audio_finetuning', 'data': '/media/SSD/tungtk2/fairseq/data//ICBHI_256_32_unnormalized_log_augmented', 'labels': 'label', 'profiling': False, 'profiles_path': 'None', 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'num_mels': 192, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': none, 'auto_encoder': False, 'sup_contrast': True, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'eval_bleu': False, 'eval_bleu_detok': None, 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_args': '{}', 'eval_bleu_print_samples': False, 'autoregressive': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True, 'class_weights': [1.0, 1.0, 1.0, 1.0]}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.004, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.05, 0.75, 0.2], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 4000000.0, 'lr': [0.0001]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-06-06 09:17:56,320][fairseq.models.wav2vec.wav2vec2_asr][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 200, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'wav2vec2_icbhi_pretrain', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15508', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 25000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 384, 'encoder_ffn_embed_dim': 768, 'encoder_attention_heads': 6, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.5, 'encoder_layerdrop': 0.5, 'dropout_input': 0.0, 'dropout_features': 0.3, 'final_dim': 384, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 5, 1, 1)] + [(512, 5, 1, 2)] + [(512, 5, 2, 2)] + [(384, 5, 4, 2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': True, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.3, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 32, 'mask_channel_prob': 0.25, 'mask_channel_before': False, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'stft_audio_pretraining', 'data': '/media/SSD/tungtk2/fairseq/data/ICBHI_256_32_unnormalized_log_augmented', 'labels': None, 'profiling': False, 'profiles_path': None, 'normalize': False, 'enable_padding': True, 'max_sample_size': 5000, 'min_sample_size': 100, 'num_mels': 192, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'inferred_w2v_config': None, 'tpu': False, 'text_compression_level': 'none'}, 'criterion': None, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': None, 'scoring': None, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2022-06-06 09:17:56,574][fairseq_cli.train][INFO] - Wav2Vec2Seq2SeqModel(
  (encoder): Wav2VecEncoder(
    (w2v_model): Wav2Vec2Model(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(192, 512, kernel_size=(5,), stride=(1,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU()
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), dilation=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (3): Sequential(
            (0): Conv1d(512, 384, kernel_size=(5,), stride=(2,), dilation=(4,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
        )
      )
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.3, inplace=False)
      (quantizer): None
      (project_q): None
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(384, 384, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU()
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=384, out_features=384, bias=True)
              (v_proj): Linear(in_features=384, out_features=384, bias=True)
              (q_proj): Linear(in_features=384, out_features=384, bias=True)
              (out_proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.5, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=384, out_features=768, bias=True)
            (fc2): Linear(in_features=768, out_features=384, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([384]), eps=1e-05, elementwise_affine=True)
      )
      (final_proj): None
    )
    (final_dropout): Dropout(p=0.5, inplace=False)
  )
  (decoder): Sequential(
    (0): Linear(in_features=384, out_features=4, bias=True)
  )
)
[2022-06-06 09:17:56,575][fairseq_cli.train][INFO] - task: AudioFinetuningTask
[2022-06-06 09:17:56,576][fairseq_cli.train][INFO] - model: Wav2Vec2Seq2SeqModel
[2022-06-06 09:17:56,576][fairseq_cli.train][INFO] - criterion: CrossEntropyCriterion
[2022-06-06 09:17:56,576][fairseq_cli.train][INFO] - num. shared model params: 19,486,340 (num. trained: 19,486,340)
[2022-06-06 09:17:56,577][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2022-06-06 09:17:56,580][fairseq.data.audio.stft_audio_dataset][INFO] - loaded 1444, skipped 0 samples
[2022-06-06 09:17:56,756][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0
[2022-06-06 09:17:56,767][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.
[2022-06-06 09:17:56,767][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
[2022-06-06 09:17:56,767][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
[2022-06-06 09:17:56,767][fairseq.trainer][INFO] - detected shared parameter: encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
[2022-06-06 09:17:56,829][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************
[2022-06-06 09:17:56,829][fairseq.utils][INFO] - rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
[2022-06-06 09:17:56,829][fairseq.utils][INFO] - rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
[2022-06-06 09:17:56,829][fairseq.utils][INFO] - ***********************CUDA enviroments for all 2 workers***********************
[2022-06-06 09:17:56,830][fairseq_cli.train][INFO] - training on 2 devices (GPUs/TPUs)
[2022-06-06 09:17:56,830][fairseq_cli.train][INFO] - max tokens per device = None and max sentences per device = 16
[2022-06-06 09:17:56,830][fairseq.trainer][INFO] - Preparing to load checkpoint checkpoints/checkpoint_last.pt
[2022-06-06 09:17:56,831][fairseq.trainer][INFO] - No existing checkpoint found checkpoints/checkpoint_last.pt
[2022-06-06 09:17:56,831][fairseq.trainer][INFO] - loading train data for epoch 1
[2022-06-06 09:17:56,928][fairseq.data.audio.stft_audio_dataset][INFO] - loaded 25370, skipped 0 samples
[2022-06-06 09:17:57,498][fairseq.trainer][INFO] - NOTE: your device may support faster training with --fp16 or --amp
[2022-06-06 09:17:57,504][fairseq.optim.adam][INFO] - using FusedAdam
[2022-06-06 09:17:57,509][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
wandb: Currently logged in as: hungbd7. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.12.17 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.16
wandb: Run data is saved locally in /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/wandb/run-20220606_091758-39mgcc02
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 2022-06-06/09-17-46
wandb: â­ï¸ View project at https://wandb.ai/hungbd7/wav2vec2_icbhi
wandb: ðŸš€ View run at https://wandb.ai/hungbd7/wav2vec2_icbhi/runs/39mgcc02
[2022-06-06 09:18:01,393][fairseq.trainer][INFO] - begin training epoch 1
[2022-06-06 09:18:01,394][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:18:44,886][train_inner][INFO] - {"epoch": 1, "update": 0.252, "loss": "0.264", "ce_loss": "0.507", "scl_loss": "0.237", "accuracy": "27.394", "ppl": "1.2", "wps": "218.2", "ups": "6.83", "wpb": "32", "bsz": "32", "num_updates": "200", "lr": "1.099e-06", "gnorm": "0.149", "train_wall": "29", "gb_free": "22.8", "wall": "48"}
[2022-06-06 09:19:13,906][train_inner][INFO] - {"epoch": 1, "update": 0.504, "loss": "0.264", "ce_loss": "0.501", "scl_loss": "0.238", "accuracy": "27.826", "ppl": "1.2", "wps": "220.1", "ups": "6.89", "wpb": "31.9", "bsz": "31.9", "num_updates": "400", "lr": "1.198e-06", "gnorm": "0.14", "train_wall": "28", "gb_free": "22.9", "wall": "77"}
[2022-06-06 09:19:42,873][train_inner][INFO] - {"epoch": 1, "update": 0.756, "loss": "0.263", "ce_loss": "0.499", "scl_loss": "0.237", "accuracy": "28.969", "ppl": "1.2", "wps": "221", "ups": "6.91", "wpb": "32", "bsz": "32", "num_updates": "600", "lr": "1.297e-06", "gnorm": "0.138", "train_wall": "28", "gb_free": "23", "wall": "106"}
[2022-06-06 09:20:10,144][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[ 55  96 572  52]
 [  2  97 239  53]
 [  3  15 154  34]
 [  0   6  61   5]]
SENSE:  0.38266068747902526
SPEC:  0.07096774193548387
[2022-06-06 09:20:28,960][valid][INFO] - {"epoch": 1, "valid_loss": "0.267", "valid_ce_loss": "0.531", "valid_scl_loss": "0.237", "valid_accuracy": "21.537", "valid_ppl": "1.2", "valid_wps": "636.6", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.226814", "valid_num_updates": "794"}
[2022-06-06 09:20:28,962][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 794 updates
[2022-06-06 09:20:28,962][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[ 55  96 572  52]
 [  2  97 239  53]
 [  3  15 154  34]
 [  0   6  61   5]]
SENSE:  0.38266068747902526
SPEC:  0.07096774193548387
[2022-06-06 09:20:30,114][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[2022-06-06 09:20:30,993][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 794 updates, score 0.22681421470725457) (writing took 2.0312002659775317 seconds)
[2022-06-06 09:20:30,993][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2022-06-06 09:20:30,995][train][INFO] - {"epoch": 1, "train_loss": "0.263", "train_ce_loss": "0.5", "train_scl_loss": "0.237", "train_accuracy": "28.412", "train_ppl": "1.2", "train_wps": "187.3", "train_ups": "5.86", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "794", "train_lr": "1.39303e-06", "train_gnorm": "0.142", "train_train_wall": "113", "train_gb_free": "23", "train_wall": "154"}
[2022-06-06 09:20:31,025][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:20:31,037][fairseq.trainer][INFO] - begin training epoch 2
[2022-06-06 09:20:31,037][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:20:47,447][train_inner][INFO] - {"epoch": 2, "update": 1.008, "loss": "0.262", "ce_loss": "0.496", "scl_loss": "0.236", "accuracy": "29.605", "ppl": "1.2", "wps": "98.9", "ups": "3.1", "wpb": "31.9", "bsz": "31.9", "num_updates": "800", "lr": "1.396e-06", "gnorm": "0.14", "train_wall": "28", "gb_free": "23", "wall": "171"}
[2022-06-06 09:21:16,388][train_inner][INFO] - {"epoch": 2, "update": 1.259, "loss": "0.262", "ce_loss": "0.493", "scl_loss": "0.236", "accuracy": "31.133", "ppl": "1.2", "wps": "221", "ups": "6.91", "wpb": "32", "bsz": "32", "num_updates": "1000", "lr": "1.495e-06", "gnorm": "0.139", "train_wall": "28", "gb_free": "23.3", "wall": "200"}
[2022-06-06 09:21:16,400][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[ 33 446  65 231]
 [  1 243   3 144]
 [  0 100   4 102]
 [  0  58   0  14]]
SENSE:  0.39013452903134993
SPEC:  0.04258064516129032
[2022-06-06 09:21:34,456][valid][INFO] - {"epoch": 2, "valid_loss": "0.268", "valid_ce_loss": "0.548", "valid_scl_loss": "0.237", "valid_accuracy": "20.36", "valid_ppl": "1.2", "valid_wps": "691.3", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.216358", "valid_num_updates": "1000", "valid_best_icbhi": "0.226814"}
[2022-06-06 09:21:34,458][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 1000 updates
[2022-06-06 09:21:34,459][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_2_1000.pt
[2022-06-06 09:21:35,673][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_2_1000.pt
[2022-06-06 09:21:36,689][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_2_1000.pt (epoch 2 @ 1000 updates, score 0.21635758709632014) (writing took 2.2309221390169114 seconds)
[2022-06-06 09:22:05,878][train_inner][INFO] - {"epoch": 2, "update": 1.511, "loss": "0.261", "ce_loss": "0.491", "scl_loss": "0.236", "accuracy": "32.524", "ppl": "1.2", "wps": "129.1", "ups": "4.04", "wpb": "31.9", "bsz": "31.9", "num_updates": "1200", "lr": "1.594e-06", "gnorm": "0.137", "train_wall": "29", "gb_free": "22.4", "wall": "249"}
[2022-06-06 09:22:34,063][train_inner][INFO] - {"epoch": 2, "update": 1.763, "loss": "0.262", "ce_loss": "0.489", "scl_loss": "0.237", "accuracy": "32.812", "ppl": "1.2", "wps": "227.1", "ups": "7.1", "wpb": "32", "bsz": "32", "num_updates": "1400", "lr": "1.693e-06", "gnorm": "0.134", "train_wall": "28", "gb_free": "23", "wall": "277"}
[2022-06-06 09:23:00,724][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[ 33 446  65 231]
 [  1 243   3 144]
 [  0 100   4 102]
 [  0  58   0  14]]
SENSE:  0.39013452903134993
SPEC:  0.04258064516129032
[775.0, 391.0, 206.0000001, 72.0000001]
[[220 267 184 104]
 [ 21 270  31  69]
 [ 18 104  33  51]
 [  5  38  23   6]]
SENSE:  0.46188340793366717
SPEC:  0.2838709677419355
[2022-06-06 09:23:17,035][valid][INFO] - {"epoch": 2, "valid_loss": "0.263", "valid_ce_loss": "0.495", "valid_scl_loss": "0.237", "valid_accuracy": "36.634", "valid_ppl": "1.2", "valid_wps": "759.6", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.372877", "valid_num_updates": "1588", "valid_best_icbhi": "0.372877"}
[2022-06-06 09:23:17,036][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 2 @ 1588 updates
[2022-06-06 09:23:17,037][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[220 267 184 104]
 [ 21 270  31  69]
 [ 18 104  33  51]
 [  5  38  23   6]]
SENSE:  0.46188340793366717
SPEC:  0.2838709677419355
[2022-06-06 09:23:18,440][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[2022-06-06 09:23:19,572][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 1588 updates, score 0.3728771878378013) (writing took 2.5355530000524595 seconds)
[2022-06-06 09:23:19,572][fairseq_cli.train][INFO] - end of epoch 2 (average epoch stats below)
[2022-06-06 09:23:19,575][train][INFO] - {"epoch": 2, "train_loss": "0.262", "train_ce_loss": "0.489", "train_scl_loss": "0.236", "train_accuracy": "32.708", "train_ppl": "1.2", "train_wps": "150.5", "train_ups": "4.71", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "1588", "train_lr": "1.78606e-06", "train_gnorm": "0.136", "train_train_wall": "112", "train_gb_free": "23.1", "train_wall": "323"}
[2022-06-06 09:23:19,603][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:23:19,613][fairseq.trainer][INFO] - begin training epoch 3
[2022-06-06 09:23:19,613][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:23:36,712][train_inner][INFO] - {"epoch": 3, "update": 2.015, "loss": "0.261", "ce_loss": "0.484", "scl_loss": "0.236", "accuracy": "34.68", "ppl": "1.2", "wps": "101.9", "ups": "3.19", "wpb": "31.9", "bsz": "31.9", "num_updates": "1600", "lr": "1.792e-06", "gnorm": "0.133", "train_wall": "28", "gb_free": "22.7", "wall": "340"}
[2022-06-06 09:24:05,169][train_inner][INFO] - {"epoch": 3, "update": 2.267, "loss": "0.261", "ce_loss": "0.481", "scl_loss": "0.236", "accuracy": "36.641", "ppl": "1.2", "wps": "224.9", "ups": "7.03", "wpb": "32", "bsz": "32", "num_updates": "1800", "lr": "1.891e-06", "gnorm": "0.135", "train_wall": "28", "gb_free": "23.1", "wall": "368"}
[2022-06-06 09:24:34,604][train_inner][INFO] - {"epoch": 3, "update": 2.519, "loss": "0.261", "ce_loss": "0.481", "scl_loss": "0.237", "accuracy": "36.375", "ppl": "1.2", "wps": "217.5", "ups": "6.8", "wpb": "32", "bsz": "32", "num_updates": "2000", "lr": "1.99e-06", "gnorm": "0.131", "train_wall": "29", "gb_free": "22.8", "wall": "398"}
[2022-06-06 09:24:34,605][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[236 284 208  47]
 [ 29 290  36  36]
 [ 21 121  45  19]
 [  6  31  30   5]]
SENSE:  0.5082212255580804
SPEC:  0.30451612903225805
[2022-06-06 09:24:52,170][valid][INFO] - {"epoch": 3, "valid_loss": "0.261", "valid_ce_loss": "0.48", "valid_scl_loss": "0.237", "valid_accuracy": "39.889", "valid_ppl": "1.2", "valid_wps": "742.4", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.406369", "valid_num_updates": "2000", "valid_best_icbhi": "0.406369"}
[2022-06-06 09:24:52,171][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 2000 updates
[2022-06-06 09:24:52,172][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_3_2000.pt
[2022-06-06 09:24:53,468][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_3_2000.pt
[2022-06-06 09:24:55,265][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_3_2000.pt (epoch 3 @ 2000 updates, score 0.40636867729516923) (writing took 3.093630908988416 seconds)
[2022-06-06 09:25:24,199][train_inner][INFO] - {"epoch": 3, "update": 2.771, "loss": "0.26", "ce_loss": "0.478", "scl_loss": "0.236", "accuracy": "37.551", "ppl": "1.2", "wps": "128.8", "ups": "4.03", "wpb": "31.9", "bsz": "31.9", "num_updates": "2200", "lr": "2.089e-06", "gnorm": "0.134", "train_wall": "28", "gb_free": "23.1", "wall": "447"}
[2022-06-06 09:25:49,951][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[236 284 208  47]
 [ 29 290  36  36]
 [ 21 121  45  19]
 [  6  31  30   5]]
SENSE:  0.5082212255580804
SPEC:  0.30451612903225805
[775.0, 391.0, 206.0000001, 72.0000001]
[[193  39 368 175]
 [ 18  66 119 188]
 [ 12   5 101  88]
 [  3   3  40  26]]
SENSE:  0.28849028391973386
SPEC:  0.24903225806451612
[2022-06-06 09:26:07,218][valid][INFO] - {"epoch": 3, "valid_loss": "0.264", "valid_ce_loss": "0.509", "valid_scl_loss": "0.237", "valid_accuracy": "26.731", "valid_ppl": "1.2", "valid_wps": "764.5", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.268761", "valid_num_updates": "2382", "valid_best_icbhi": "0.406369"}
[2022-06-06 09:26:07,220][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 3 @ 2382 updates
[2022-06-06 09:26:07,221][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[193  39 368 175]
 [ 18  66 119 188]
 [ 12   5 101  88]
 [  3   3  40  26]]
SENSE:  0.28849028391973386
SPEC:  0.24903225806451612
[2022-06-06 09:26:08,388][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:26:08,397][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 3 @ 2382 updates, score 0.268761270992125) (writing took 1.1763230569195002 seconds)
[2022-06-06 09:26:08,397][fairseq_cli.train][INFO] - end of epoch 3 (average epoch stats below)
[2022-06-06 09:26:08,400][train][INFO] - {"epoch": 3, "train_loss": "0.26", "train_ce_loss": "0.479", "train_scl_loss": "0.236", "train_accuracy": "37.103", "train_ppl": "1.2", "train_wps": "150.3", "train_ups": "4.7", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "2382", "train_lr": "2.17909e-06", "train_gnorm": "0.134", "train_train_wall": "112", "train_gb_free": "22.9", "train_wall": "492"}
[2022-06-06 09:26:08,426][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:26:08,439][fairseq.trainer][INFO] - begin training epoch 4
[2022-06-06 09:26:08,439][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:26:25,800][train_inner][INFO] - {"epoch": 4, "update": 3.023, "loss": "0.259", "ce_loss": "0.474", "scl_loss": "0.235", "accuracy": "37.861", "ppl": "1.2", "wps": "103.5", "ups": "3.25", "wpb": "31.9", "bsz": "31.9", "num_updates": "2400", "lr": "2.188e-06", "gnorm": "0.136", "train_wall": "28", "gb_free": "23.1", "wall": "509"}
[2022-06-06 09:26:54,919][train_inner][INFO] - {"epoch": 4, "update": 3.275, "loss": "0.259", "ce_loss": "0.472", "scl_loss": "0.235", "accuracy": "38.694", "ppl": "1.2", "wps": "219.3", "ups": "6.87", "wpb": "31.9", "bsz": "31.9", "num_updates": "2600", "lr": "2.287e-06", "gnorm": "0.136", "train_wall": "28", "gb_free": "23", "wall": "538"}
[2022-06-06 09:27:23,374][train_inner][INFO] - {"epoch": 4, "update": 3.526, "loss": "0.258", "ce_loss": "0.468", "scl_loss": "0.235", "accuracy": "40.703", "ppl": "1.2", "wps": "224.9", "ups": "7.03", "wpb": "32", "bsz": "32", "num_updates": "2800", "lr": "2.386e-06", "gnorm": "0.139", "train_wall": "28", "gb_free": "23", "wall": "567"}
[2022-06-06 09:27:53,120][train_inner][INFO] - {"epoch": 4, "update": 3.778, "loss": "0.258", "ce_loss": "0.465", "scl_loss": "0.235", "accuracy": "40.785", "ppl": "1.2", "wps": "214.9", "ups": "6.72", "wpb": "32", "bsz": "32", "num_updates": "3000", "lr": "2.485e-06", "gnorm": "0.141", "train_wall": "29", "gb_free": "22.6", "wall": "596"}
[2022-06-06 09:27:53,121][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[306 277 164  28]
 [ 46 298  23  24]
 [ 42 117  39   8]
 [ 10  33  25   4]]
SENSE:  0.5097159938685454
SPEC:  0.39483870967741935
[2022-06-06 09:28:08,965][valid][INFO] - {"epoch": 4, "valid_loss": "0.263", "valid_ce_loss": "0.477", "valid_scl_loss": "0.24", "valid_accuracy": "44.806", "valid_ppl": "1.2", "valid_wps": "741.5", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.452277", "valid_num_updates": "3000", "valid_best_icbhi": "0.452277"}
[2022-06-06 09:28:08,967][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 3000 updates
[2022-06-06 09:28:08,967][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_4_3000.pt
[2022-06-06 09:28:10,256][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_4_3000.pt
[2022-06-06 09:28:12,655][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_4_3000.pt (epoch 4 @ 3000 updates, score 0.45227735177298234) (writing took 3.688526517013088 seconds)
[2022-06-06 09:28:38,456][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[306 277 164  28]
 [ 46 298  23  24]
 [ 42 117  39   8]
 [ 10  33  25   4]]
SENSE:  0.5097159938685454
SPEC:  0.39483870967741935
[775.0, 391.0, 206.0000001, 72.0000001]
[[216 200 297  62]
 [ 26 239  75  51]
 [ 22  74  74  36]
 [  2  24  36  10]]
SENSE:  0.48281016428017637
SPEC:  0.2787096774193548
[2022-06-06 09:28:54,158][valid][INFO] - {"epoch": 4, "valid_loss": "0.269", "valid_ce_loss": "0.523", "valid_scl_loss": "0.241", "valid_accuracy": "37.327", "valid_ppl": "1.21", "valid_wps": "749.1", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.38076", "valid_num_updates": "3176", "valid_best_icbhi": "0.452277"}
[2022-06-06 09:28:54,159][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 4 @ 3176 updates
[2022-06-06 09:28:54,160][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[216 200 297  62]
 [ 26 239  75  51]
 [ 22  74  74  36]
 [  2  24  36  10]]
SENSE:  0.48281016428017637
SPEC:  0.2787096774193548
[2022-06-06 09:28:55,574][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:28:55,583][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 4 @ 3176 updates, score 0.3807599208497656) (writing took 1.4233992220833898 seconds)
[2022-06-06 09:28:55,583][fairseq_cli.train][INFO] - end of epoch 4 (average epoch stats below)
[2022-06-06 09:28:55,585][train][INFO] - {"epoch": 4, "train_loss": "0.258", "train_ce_loss": "0.467", "train_scl_loss": "0.235", "train_accuracy": "40.304", "train_ppl": "1.2", "train_wps": "151.7", "train_ups": "4.75", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "3176", "train_lr": "2.57212e-06", "train_gnorm": "0.142", "train_train_wall": "113", "train_gb_free": "23", "train_wall": "659"}
[2022-06-06 09:28:55,628][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:28:55,643][fairseq.trainer][INFO] - begin training epoch 5
[2022-06-06 09:28:55,643][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:29:13,613][train_inner][INFO] - {"epoch": 5, "update": 4.03, "loss": "0.258", "ce_loss": "0.464", "scl_loss": "0.235", "accuracy": "41.259", "ppl": "1.2", "wps": "79.3", "ups": "2.48", "wpb": "31.9", "bsz": "31.9", "num_updates": "3200", "lr": "2.584e-06", "gnorm": "0.153", "train_wall": "29", "gb_free": "22.7", "wall": "677"}
[2022-06-06 09:29:42,109][train_inner][INFO] - {"epoch": 5, "update": 4.282, "loss": "0.257", "ce_loss": "0.461", "scl_loss": "0.234", "accuracy": "41.266", "ppl": "1.19", "wps": "224.6", "ups": "7.02", "wpb": "32", "bsz": "32", "num_updates": "3400", "lr": "2.683e-06", "gnorm": "0.158", "train_wall": "28", "gb_free": "22.8", "wall": "705"}
[2022-06-06 09:30:10,405][train_inner][INFO] - {"epoch": 5, "update": 4.534, "loss": "0.256", "ce_loss": "0.455", "scl_loss": "0.234", "accuracy": "43.815", "ppl": "1.19", "wps": "225.9", "ups": "7.07", "wpb": "31.9", "bsz": "31.9", "num_updates": "3600", "lr": "2.782e-06", "gnorm": "0.163", "train_wall": "28", "gb_free": "22.9", "wall": "734"}
[2022-06-06 09:30:40,610][train_inner][INFO] - {"epoch": 5, "update": 4.786, "loss": "0.256", "ce_loss": "0.454", "scl_loss": "0.234", "accuracy": "42.781", "ppl": "1.19", "wps": "211.9", "ups": "6.62", "wpb": "32", "bsz": "32", "num_updates": "3800", "lr": "2.881e-06", "gnorm": "0.174", "train_wall": "30", "gb_free": "23", "wall": "764"}
[2022-06-06 09:31:04,879][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[271 275 209  20]
 [ 47 299  31  14]
 [ 46 109  41  10]
 [  3  36  31   2]]
SENSE:  0.5112107621790103
SPEC:  0.3496774193548387
[2022-06-06 09:31:20,812][valid][INFO] - {"epoch": 5, "valid_loss": "0.277", "valid_ce_loss": "0.526", "valid_scl_loss": "0.249", "valid_accuracy": "42.452", "valid_ppl": "1.21", "valid_wps": "757.8", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.430444", "valid_num_updates": "3970", "valid_best_icbhi": "0.452277"}
[2022-06-06 09:31:20,814][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 5 @ 3970 updates
[2022-06-06 09:31:20,814][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[271 275 209  20]
 [ 47 299  31  14]
 [ 46 109  41  10]
 [  3  36  31   2]]
SENSE:  0.5112107621790103
SPEC:  0.3496774193548387
[2022-06-06 09:31:22,298][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:31:22,313][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 5 @ 3970 updates, score 0.43044409076692447) (writing took 1.4996783009264618 seconds)
[2022-06-06 09:31:22,314][fairseq_cli.train][INFO] - end of epoch 5 (average epoch stats below)
[2022-06-06 09:31:22,318][train][INFO] - {"epoch": 5, "train_loss": "0.256", "train_ce_loss": "0.455", "train_scl_loss": "0.234", "train_accuracy": "43.074", "train_ppl": "1.19", "train_wps": "172.9", "train_ups": "5.41", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "3970", "train_lr": "2.96515e-06", "train_gnorm": "0.166", "train_train_wall": "112", "train_gb_free": "23", "train_wall": "805"}
[2022-06-06 09:31:22,353][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:31:22,373][fairseq.trainer][INFO] - begin training epoch 6
[2022-06-06 09:31:22,374][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:31:40,912][train_inner][INFO] - {"epoch": 6, "update": 5.038, "loss": "0.256", "ce_loss": "0.448", "scl_loss": "0.234", "accuracy": "44.793", "ppl": "1.19", "wps": "105.7", "ups": "3.32", "wpb": "31.9", "bsz": "31.9", "num_updates": "4000", "lr": "2.98e-06", "gnorm": "0.172", "train_wall": "28", "gb_free": "22.9", "wall": "824"}
[2022-06-06 09:31:40,913][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[256 276 181  62]
 [ 30 291  21  49]
 [ 40  96  34  36]
 [  4  28  28  12]]
SENSE:  0.5037369206266856
SPEC:  0.3303225806451613
[2022-06-06 09:31:56,658][valid][INFO] - {"epoch": 6, "valid_loss": "0.282", "valid_ce_loss": "0.563", "valid_scl_loss": "0.25", "valid_accuracy": "41.066", "valid_ppl": "1.22", "valid_wps": "752.5", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.41703", "valid_num_updates": "4000", "valid_best_icbhi": "0.452277"}
[2022-06-06 09:31:56,659][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 4000 updates
[2022-06-06 09:31:56,660][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_6_4000.pt
[2022-06-06 09:31:57,817][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_6_4000.pt
[2022-06-06 09:31:58,729][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_6_4000.pt (epoch 6 @ 4000 updates, score 0.41702975063592346) (writing took 2.0694001599913463 seconds)
[2022-06-06 09:32:26,775][train_inner][INFO] - {"epoch": 6, "update": 5.29, "loss": "0.254", "ce_loss": "0.438", "scl_loss": "0.233", "accuracy": "47.15", "ppl": "1.19", "wps": "139.2", "ups": "4.36", "wpb": "31.9", "bsz": "31.9", "num_updates": "4200", "lr": "3.079e-06", "gnorm": "0.188", "train_wall": "27", "gb_free": "23", "wall": "870"}
[2022-06-06 09:32:56,571][train_inner][INFO] - {"epoch": 6, "update": 5.542, "loss": "0.254", "ce_loss": "0.445", "scl_loss": "0.232", "accuracy": "45.062", "ppl": "1.19", "wps": "214.8", "ups": "6.71", "wpb": "32", "bsz": "32", "num_updates": "4400", "lr": "3.178e-06", "gnorm": "0.208", "train_wall": "29", "gb_free": "22.5", "wall": "900"}
[2022-06-06 09:33:26,245][train_inner][INFO] - {"epoch": 6, "update": 5.793, "loss": "0.253", "ce_loss": "0.438", "scl_loss": "0.233", "accuracy": "45.87", "ppl": "1.19", "wps": "215.4", "ups": "6.74", "wpb": "32", "bsz": "32", "num_updates": "4600", "lr": "3.277e-06", "gnorm": "0.227", "train_wall": "29", "gb_free": "23", "wall": "929"}
[2022-06-06 09:33:49,870][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[256 276 181  62]
 [ 30 291  21  49]
 [ 40  96  34  36]
 [  4  28  28  12]]
SENSE:  0.5037369206266856
SPEC:  0.3303225806451613
[775.0, 391.0, 206.0000001, 72.0000001]
[[243 259 235  38]
 [ 51 274  27  39]
 [ 47  84  48  27]
 [  2  27  31  12]]
SENSE:  0.4992526156952907
SPEC:  0.3135483870967742
[2022-06-06 09:34:04,852][valid][INFO] - {"epoch": 6, "valid_loss": "0.298", "valid_ce_loss": "0.627", "valid_scl_loss": "0.261", "valid_accuracy": "39.958", "valid_ppl": "1.23", "valid_wps": "542.2", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.406401", "valid_num_updates": "4764", "valid_best_icbhi": "0.452277"}
[2022-06-06 09:34:04,854][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 6 @ 4764 updates
[2022-06-06 09:34:04,855][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[243 259 235  38]
 [ 51 274  27  39]
 [ 47  84  48  27]
 [  2  27  31  12]]
SENSE:  0.4992526156952907
SPEC:  0.3135483870967742
[2022-06-06 09:34:06,340][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:34:06,348][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 6 @ 4764 updates, score 0.40640050139603245) (writing took 1.4936354950768873 seconds)
[2022-06-06 09:34:06,348][fairseq_cli.train][INFO] - end of epoch 6 (average epoch stats below)
[2022-06-06 09:34:06,351][train][INFO] - {"epoch": 6, "train_loss": "0.253", "train_ce_loss": "0.44", "train_scl_loss": "0.233", "train_accuracy": "46.05", "train_ppl": "1.19", "train_wps": "154.7", "train_ups": "4.84", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "4764", "train_lr": "3.35818e-06", "train_gnorm": "0.214", "train_train_wall": "113", "train_gb_free": "22.7", "train_wall": "970"}
[2022-06-06 09:34:06,380][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:34:06,395][fairseq.trainer][INFO] - begin training epoch 7
[2022-06-06 09:34:06,395][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:34:26,086][train_inner][INFO] - {"epoch": 7, "update": 6.045, "loss": "0.251", "ce_loss": "0.434", "scl_loss": "0.231", "accuracy": "47.008", "ppl": "1.19", "wps": "106.7", "ups": "3.34", "wpb": "31.9", "bsz": "31.9", "num_updates": "4800", "lr": "3.376e-06", "gnorm": "0.246", "train_wall": "29", "gb_free": "23", "wall": "989"}
[2022-06-06 09:34:54,781][train_inner][INFO] - {"epoch": 7, "update": 6.297, "loss": "0.251", "ce_loss": "0.43", "scl_loss": "0.231", "accuracy": "48.234", "ppl": "1.19", "wps": "223.1", "ups": "6.97", "wpb": "32", "bsz": "32", "num_updates": "5000", "lr": "3.475e-06", "gnorm": "0.256", "train_wall": "28", "gb_free": "23.1", "wall": "1018"}
[2022-06-06 09:34:54,782][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[293 229 215  38]
 [ 61 260  36  34]
 [ 49  75  54  28]
 [  2  25  35  10]]
SENSE:  0.48430493259064133
SPEC:  0.37806451612903225
[2022-06-06 09:35:10,232][valid][INFO] - {"epoch": 7, "valid_loss": "0.297", "valid_ce_loss": "0.593", "valid_scl_loss": "0.264", "valid_accuracy": "42.729", "valid_ppl": "1.23", "valid_wps": "535.6", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.431185", "valid_num_updates": "5000", "valid_best_icbhi": "0.452277"}
[2022-06-06 09:35:10,235][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 5000 updates
[2022-06-06 09:35:10,236][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_7_5000.pt
[2022-06-06 09:35:11,271][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_7_5000.pt
[2022-06-06 09:35:12,159][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_7_5000.pt (epoch 7 @ 5000 updates, score 0.4311847243598368) (writing took 1.9239292319398373 seconds)
[2022-06-06 09:35:41,310][train_inner][INFO] - {"epoch": 7, "update": 6.549, "loss": "0.251", "ce_loss": "0.425", "scl_loss": "0.231", "accuracy": "49.202", "ppl": "1.19", "wps": "137.4", "ups": "4.3", "wpb": "32", "bsz": "32", "num_updates": "5200", "lr": "3.574e-06", "gnorm": "0.278", "train_wall": "29", "gb_free": "23.1", "wall": "1064"}
[2022-06-06 09:36:11,135][train_inner][INFO] - {"epoch": 7, "update": 6.801, "loss": "0.25", "ce_loss": "0.421", "scl_loss": "0.231", "accuracy": "49.139", "ppl": "1.19", "wps": "214.1", "ups": "6.71", "wpb": "31.9", "bsz": "31.9", "num_updates": "5400", "lr": "3.673e-06", "gnorm": "0.315", "train_wall": "29", "gb_free": "23", "wall": "1094"}
[2022-06-06 09:36:33,558][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[293 229 215  38]
 [ 61 260  36  34]
 [ 49  75  54  28]
 [  2  25  35  10]]
SENSE:  0.48430493259064133
SPEC:  0.37806451612903225
[775.0, 391.0, 206.0000001, 72.0000001]
[[313 257 166  39]
 [ 62 273  26  30]
 [ 52  78  51  25]
 [  2  26  34  10]]
SENSE:  0.4992526156952907
SPEC:  0.4038709677419355
[2022-06-06 09:36:48,551][valid][INFO] - {"epoch": 7, "valid_loss": "0.296", "valid_ce_loss": "0.591", "valid_scl_loss": "0.263", "valid_accuracy": "44.806", "valid_ppl": "1.23", "valid_wps": "982.1", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.451562", "valid_num_updates": "5558", "valid_best_icbhi": "0.452277"}
[2022-06-06 09:36:48,553][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 7 @ 5558 updates
[2022-06-06 09:36:48,554][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[313 257 166  39]
 [ 62 273  26  30]
 [ 52  78  51  25]
 [  2  26  34  10]]
SENSE:  0.4992526156952907
SPEC:  0.4038709677419355
[2022-06-06 09:36:51,100][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:36:51,116][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 7 @ 5558 updates, score 0.4515617917186131) (writing took 2.5632144879782572 seconds)
[2022-06-06 09:36:51,124][fairseq_cli.train][INFO] - end of epoch 7 (average epoch stats below)
[2022-06-06 09:36:51,129][train][INFO] - {"epoch": 7, "train_loss": "0.25", "train_ce_loss": "0.424", "train_scl_loss": "0.231", "train_accuracy": "48.959", "train_ppl": "1.19", "train_wps": "154", "train_ups": "4.82", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "5558", "train_lr": "3.75121e-06", "train_gnorm": "0.292", "train_train_wall": "113", "train_gb_free": "22.8", "train_wall": "1134"}
[2022-06-06 09:36:51,704][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:36:51,727][fairseq.trainer][INFO] - begin training epoch 8
[2022-06-06 09:36:51,727][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:37:11,471][train_inner][INFO] - {"epoch": 8, "update": 7.053, "loss": "0.248", "ce_loss": "0.419", "scl_loss": "0.229", "accuracy": "49.655", "ppl": "1.19", "wps": "105.6", "ups": "3.31", "wpb": "31.9", "bsz": "31.9", "num_updates": "5600", "lr": "3.772e-06", "gnorm": "0.33", "train_wall": "28", "gb_free": "22.9", "wall": "1155"}
[2022-06-06 09:37:40,366][train_inner][INFO] - {"epoch": 8, "update": 7.305, "loss": "0.247", "ce_loss": "0.411", "scl_loss": "0.229", "accuracy": "51.672", "ppl": "1.19", "wps": "221.5", "ups": "6.92", "wpb": "32", "bsz": "32", "num_updates": "5800", "lr": "3.871e-06", "gnorm": "0.344", "train_wall": "28", "gb_free": "22.9", "wall": "1184"}
[2022-06-06 09:38:10,052][train_inner][INFO] - {"epoch": 8, "update": 7.557, "loss": "0.247", "ce_loss": "0.408", "scl_loss": "0.229", "accuracy": "51.953", "ppl": "1.19", "wps": "215.6", "ups": "6.74", "wpb": "32", "bsz": "32", "num_updates": "6000", "lr": "3.97e-06", "gnorm": "0.387", "train_wall": "29", "gb_free": "22.7", "wall": "1213"}
[2022-06-06 09:38:10,053][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[339 258 149  29]
 [ 75 268  26  22]
 [ 55  81  51  19]
 [  4  27  37   4]]
SENSE:  0.48281016428017637
SPEC:  0.4374193548387097
[2022-06-06 09:38:25,680][valid][INFO] - {"epoch": 8, "valid_loss": "0.298", "valid_ce_loss": "0.606", "valid_scl_loss": "0.263", "valid_accuracy": "45.845", "valid_ppl": "1.23", "valid_wps": "741.3", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.460115", "valid_num_updates": "6000", "valid_best_icbhi": "0.460115"}
[2022-06-06 09:38:25,683][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 6000 updates
[2022-06-06 09:38:25,686][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_8_6000.pt
[2022-06-06 09:38:27,314][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_8_6000.pt
[2022-06-06 09:38:30,355][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_8_6000.pt (epoch 8 @ 6000 updates, score 0.46011475955944303) (writing took 4.671951364958659 seconds)
[2022-06-06 09:38:59,792][train_inner][INFO] - {"epoch": 8, "update": 7.809, "loss": "0.247", "ce_loss": "0.409", "scl_loss": "0.229", "accuracy": "50.562", "ppl": "1.19", "wps": "128.7", "ups": "4.02", "wpb": "32", "bsz": "32", "num_updates": "6200", "lr": "4.069e-06", "gnorm": "0.387", "train_wall": "29", "gb_free": "22.6", "wall": "1263"}
[2022-06-06 09:39:20,883][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[339 258 149  29]
 [ 75 268  26  22]
 [ 55  81  51  19]
 [  4  27  37   4]]
SENSE:  0.48281016428017637
SPEC:  0.4374193548387097
[775.0, 391.0, 206.0000001, 72.0000001]
[[419 271  73  12]
 [ 89 284   9   9]
 [ 67  91  34  14]
 [  9  31  27   5]]
SENSE:  0.48281016428017637
SPEC:  0.5406451612903226
[2022-06-06 09:39:37,959][valid][INFO] - {"epoch": 8, "valid_loss": "0.29", "valid_ce_loss": "0.59", "valid_scl_loss": "0.257", "valid_accuracy": "51.385", "valid_ppl": "1.22", "valid_wps": "515.1", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.511728", "valid_num_updates": "6352", "valid_best_icbhi": "0.511728"}
[2022-06-06 09:39:37,961][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 8 @ 6352 updates
[2022-06-06 09:39:37,963][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[419 271  73  12]
 [ 89 284   9   9]
 [ 67  91  34  14]
 [  9  31  27   5]]
SENSE:  0.48281016428017637
SPEC:  0.5406451612903226
[2022-06-06 09:39:39,490][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[2022-06-06 09:39:40,707][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 6352 updates, score 0.5117276627852495) (writing took 2.7453960520215333 seconds)
[2022-06-06 09:39:40,707][fairseq_cli.train][INFO] - end of epoch 8 (average epoch stats below)
[2022-06-06 09:39:40,710][train][INFO] - {"epoch": 8, "train_loss": "0.246", "train_ce_loss": "0.406", "train_scl_loss": "0.228", "train_accuracy": "51.825", "train_ppl": "1.19", "train_wps": "149.6", "train_ups": "4.68", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "6352", "train_lr": "4.14424e-06", "train_gnorm": "0.383", "train_train_wall": "113", "train_gb_free": "22.9", "train_wall": "1304"}
[2022-06-06 09:39:40,768][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:39:40,791][fairseq.trainer][INFO] - begin training epoch 9
[2022-06-06 09:39:40,792][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:40:01,775][train_inner][INFO] - {"epoch": 9, "update": 8.06, "loss": "0.243", "ce_loss": "0.393", "scl_loss": "0.227", "accuracy": "54.391", "ppl": "1.18", "wps": "102.9", "ups": "3.23", "wpb": "31.9", "bsz": "31.9", "num_updates": "6400", "lr": "4.168e-06", "gnorm": "0.452", "train_wall": "28", "gb_free": "22.9", "wall": "1325"}
[2022-06-06 09:40:30,714][train_inner][INFO] - {"epoch": 9, "update": 8.312, "loss": "0.242", "ce_loss": "0.385", "scl_loss": "0.227", "accuracy": "54.897", "ppl": "1.18", "wps": "220.9", "ups": "6.91", "wpb": "32", "bsz": "32", "num_updates": "6600", "lr": "4.267e-06", "gnorm": "0.467", "train_wall": "28", "gb_free": "22.8", "wall": "1354"}
[2022-06-06 09:40:59,028][train_inner][INFO] - {"epoch": 9, "update": 8.564, "loss": "0.244", "ce_loss": "0.397", "scl_loss": "0.226", "accuracy": "52.646", "ppl": "1.18", "wps": "225.6", "ups": "7.06", "wpb": "31.9", "bsz": "31.9", "num_updates": "6800", "lr": "4.366e-06", "gnorm": "0.517", "train_wall": "28", "gb_free": "22.9", "wall": "1382"}
[2022-06-06 09:41:28,332][train_inner][INFO] - {"epoch": 9, "update": 8.816, "loss": "0.243", "ce_loss": "0.392", "scl_loss": "0.227", "accuracy": "55.016", "ppl": "1.18", "wps": "218.5", "ups": "6.83", "wpb": "32", "bsz": "32", "num_updates": "7000", "lr": "4.465e-06", "gnorm": "0.489", "train_wall": "29", "gb_free": "23.2", "wall": "1412"}
[2022-06-06 09:41:28,333][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[299 294 144  38]
 [ 62 276  21  32]
 [ 50  70  44  42]
 [  4  23  34  11]]
SENSE:  0.49476831076389594
SPEC:  0.3858064516129032
[2022-06-06 09:41:43,919][valid][INFO] - {"epoch": 9, "valid_loss": "0.314", "valid_ce_loss": "0.711", "valid_scl_loss": "0.27", "valid_accuracy": "43.629", "valid_ppl": "1.24", "valid_wps": "751.5", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.440287", "valid_num_updates": "7000", "valid_best_icbhi": "0.511728"}
[2022-06-06 09:41:43,921][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 7000 updates
[2022-06-06 09:41:43,922][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_9_7000.pt
[2022-06-06 09:41:45,285][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_9_7000.pt
[2022-06-06 09:41:46,350][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_9_7000.pt (epoch 9 @ 7000 updates, score 0.4402873811883996) (writing took 2.4291417149361223 seconds)
[2022-06-06 09:42:07,418][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[299 294 144  38]
 [ 62 276  21  32]
 [ 50  70  44  42]
 [  4  23  34  11]]
SENSE:  0.49476831076389594
SPEC:  0.3858064516129032
[775.0, 391.0, 206.0000001, 72.0000001]
[[308 246 170  51]
 [ 69 257  26  39]
 [ 52  58  47  49]
 [  4  20  34  14]]
SENSE:  0.47533632272785165
SPEC:  0.39741935483870966
[2022-06-06 09:42:24,240][valid][INFO] - {"epoch": 9, "valid_loss": "0.323", "valid_ce_loss": "0.747", "valid_scl_loss": "0.276", "valid_accuracy": "43.352", "valid_ppl": "1.25", "valid_wps": "554.2", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.436378", "valid_num_updates": "7146", "valid_best_icbhi": "0.511728"}
[2022-06-06 09:42:24,242][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 9 @ 7146 updates
[2022-06-06 09:42:24,242][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[308 246 170  51]
 [ 69 257  26  39]
 [ 52  58  47  49]
 [  4  20  34  14]]
SENSE:  0.47533632272785165
SPEC:  0.39741935483870966
[2022-06-06 09:42:25,560][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:42:25,569][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 9 @ 7146 updates, score 0.4363778387832806) (writing took 1.3271456130314618 seconds)
[2022-06-06 09:42:25,569][fairseq_cli.train][INFO] - end of epoch 9 (average epoch stats below)
[2022-06-06 09:42:25,571][train][INFO] - {"epoch": 9, "train_loss": "0.243", "train_ce_loss": "0.389", "train_scl_loss": "0.226", "train_accuracy": "54.84", "train_ppl": "1.18", "train_wps": "153.9", "train_ups": "4.82", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "7146", "train_lr": "4.53727e-06", "train_gnorm": "0.5", "train_train_wall": "113", "train_gb_free": "23.1", "train_wall": "1469"}
[2022-06-06 09:42:25,598][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:42:25,618][fairseq.trainer][INFO] - begin training epoch 10
[2022-06-06 09:42:25,618][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:42:47,542][train_inner][INFO] - {"epoch": 10, "update": 9.068, "loss": "0.24", "ce_loss": "0.379", "scl_loss": "0.224", "accuracy": "56.908", "ppl": "1.18", "wps": "80.6", "ups": "2.53", "wpb": "31.9", "bsz": "31.9", "num_updates": "7200", "lr": "4.564e-06", "gnorm": "0.535", "train_wall": "28", "gb_free": "22.7", "wall": "1491"}
[2022-06-06 09:43:16,085][train_inner][INFO] - {"epoch": 10, "update": 9.32, "loss": "0.24", "ce_loss": "0.375", "scl_loss": "0.225", "accuracy": "57.109", "ppl": "1.18", "wps": "223.7", "ups": "7.01", "wpb": "31.9", "bsz": "31.9", "num_updates": "7400", "lr": "4.663e-06", "gnorm": "0.592", "train_wall": "28", "gb_free": "23.1", "wall": "1519"}
[2022-06-06 09:43:44,828][train_inner][INFO] - {"epoch": 10, "update": 9.572, "loss": "0.239", "ce_loss": "0.376", "scl_loss": "0.224", "accuracy": "57.125", "ppl": "1.18", "wps": "222.7", "ups": "6.96", "wpb": "32", "bsz": "32", "num_updates": "7600", "lr": "4.762e-06", "gnorm": "0.601", "train_wall": "28", "gb_free": "22.8", "wall": "1548"}
[2022-06-06 09:44:14,672][train_inner][INFO] - {"epoch": 10, "update": 9.824, "loss": "0.239", "ce_loss": "0.374", "scl_loss": "0.224", "accuracy": "56.812", "ppl": "1.18", "wps": "214.5", "ups": "6.7", "wpb": "32", "bsz": "32", "num_updates": "7800", "lr": "4.861e-06", "gnorm": "0.601", "train_wall": "29", "gb_free": "23.1", "wall": "1578"}
[2022-06-06 09:44:35,757][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[458 228  69  20]
 [107 259  12  13]
 [ 69  69  42  26]
 [  9  26  31   6]]
SENSE:  0.4588938713127373
SPEC:  0.5909677419354838
[2022-06-06 09:44:50,966][valid][INFO] - {"epoch": 10, "valid_loss": "0.293", "valid_ce_loss": "0.623", "valid_scl_loss": "0.256", "valid_accuracy": "52.978", "valid_ppl": "1.23", "valid_wps": "857.1", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.524931", "valid_num_updates": "7940", "valid_best_icbhi": "0.524931"}
[2022-06-06 09:44:50,969][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 10 @ 7940 updates
[2022-06-06 09:44:50,969][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[2022-06-06 09:44:52,631][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[458 228  69  20]
 [107 259  12  13]
 [ 69  69  42  26]
 [  9  26  31   6]]
SENSE:  0.4588938713127373
SPEC:  0.5909677419354838
[2022-06-06 09:44:54,268][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 10 @ 7940 updates, score 0.5249308066241105) (writing took 3.2994760619476438 seconds)
[2022-06-06 09:44:54,268][fairseq_cli.train][INFO] - end of epoch 10 (average epoch stats below)
[2022-06-06 09:44:54,272][train][INFO] - {"epoch": 10, "train_loss": "0.239", "train_ce_loss": "0.375", "train_scl_loss": "0.224", "train_accuracy": "57.087", "train_ppl": "1.18", "train_wps": "170.6", "train_ups": "5.34", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "7940", "train_lr": "4.9303e-06", "train_gnorm": "0.602", "train_train_wall": "114", "train_gb_free": "23", "train_wall": "1617"}
[2022-06-06 09:44:54,301][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:44:54,318][fairseq.trainer][INFO] - begin training epoch 11
[2022-06-06 09:44:54,319][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:45:17,265][train_inner][INFO] - {"epoch": 11, "update": 10.076, "loss": "0.239", "ce_loss": "0.375", "scl_loss": "0.224", "accuracy": "58.197", "ppl": "1.18", "wps": "101.7", "ups": "3.2", "wpb": "31.8", "bsz": "31.8", "num_updates": "8000", "lr": "4.96e-06", "gnorm": "0.658", "train_wall": "30", "gb_free": "22.9", "wall": "1640"}
[2022-06-06 09:45:17,267][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[374 231 143  27]
 [ 88 265  25  13]
 [ 56  61  57  32]
 [  6  22  37   7]]
SENSE:  0.49177877414296606
SPEC:  0.48258064516129034
[2022-06-06 09:45:34,882][valid][INFO] - {"epoch": 11, "valid_loss": "0.306", "valid_ce_loss": "0.676", "valid_scl_loss": "0.265", "valid_accuracy": "48.684", "valid_ppl": "1.24", "valid_wps": "719.1", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.48718", "valid_num_updates": "8000", "valid_best_icbhi": "0.524931"}
[2022-06-06 09:45:34,884][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 8000 updates
[2022-06-06 09:45:34,885][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_11_8000.pt
[2022-06-06 09:45:36,235][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_11_8000.pt
[2022-06-06 09:45:37,382][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_11_8000.pt (epoch 11 @ 8000 updates, score 0.4871797096521282) (writing took 2.497320884023793 seconds)
[2022-06-06 09:46:07,011][train_inner][INFO] - {"epoch": 11, "update": 10.327, "loss": "0.236", "ce_loss": "0.362", "scl_loss": "0.222", "accuracy": "58.922", "ppl": "1.18", "wps": "128.7", "ups": "4.02", "wpb": "32", "bsz": "32", "num_updates": "8200", "lr": "5.059e-06", "gnorm": "0.679", "train_wall": "29", "gb_free": "23", "wall": "1690"}
[2022-06-06 09:46:38,166][train_inner][INFO] - {"epoch": 11, "update": 10.579, "loss": "0.237", "ce_loss": "0.366", "scl_loss": "0.222", "accuracy": "58.722", "ppl": "1.18", "wps": "205", "ups": "6.42", "wpb": "31.9", "bsz": "31.9", "num_updates": "8400", "lr": "5.158e-06", "gnorm": "0.696", "train_wall": "31", "gb_free": "23.2", "wall": "1721"}
[2022-06-06 09:47:09,213][train_inner][INFO] - {"epoch": 11, "update": 10.831, "loss": "0.235", "ce_loss": "0.362", "scl_loss": "0.221", "accuracy": "58.703", "ppl": "1.18", "wps": "206.2", "ups": "6.44", "wpb": "32", "bsz": "32", "num_updates": "8600", "lr": "5.257e-06", "gnorm": "0.749", "train_wall": "31", "gb_free": "23", "wall": "1752"}
[2022-06-06 09:47:29,193][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[374 231 143  27]
 [ 88 265  25  13]
 [ 56  61  57  32]
 [  6  22  37   7]]
SENSE:  0.49177877414296606
SPEC:  0.48258064516129034
[775.0, 391.0, 206.0000001, 72.0000001]
[[373 171 172  59]
 [ 93 206  43  49]
 [ 54  27  72  53]
 [  5  14  40  13]]
SENSE:  0.4349775783452982
SPEC:  0.48129032258064514
[2022-06-06 09:47:43,222][valid][INFO] - {"epoch": 11, "valid_loss": "0.318", "valid_ce_loss": "0.729", "valid_scl_loss": "0.272", "valid_accuracy": "45.983", "valid_ppl": "1.25", "valid_wps": "756.6", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.458134", "valid_num_updates": "8734", "valid_best_icbhi": "0.524931"}
[2022-06-06 09:47:43,223][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 11 @ 8734 updates
[2022-06-06 09:47:43,224][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[373 171 172  59]
 [ 93 206  43  49]
 [ 54  27  72  53]
 [  5  14  40  13]]
SENSE:  0.4349775783452982
SPEC:  0.48129032258064514
[2022-06-06 09:47:44,554][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:47:44,563][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 11 @ 8734 updates, score 0.4581339504629717) (writing took 1.3396916089113802 seconds)
[2022-06-06 09:47:44,563][fairseq_cli.train][INFO] - end of epoch 11 (average epoch stats below)
[2022-06-06 09:47:44,566][train][INFO] - {"epoch": 11, "train_loss": "0.236", "train_ce_loss": "0.363", "train_scl_loss": "0.222", "train_accuracy": "59.007", "train_ppl": "1.18", "train_wps": "149", "train_ups": "4.66", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "8734", "train_lr": "5.32333e-06", "train_gnorm": "0.715", "train_train_wall": "120", "train_gb_free": "23.2", "train_wall": "1788"}
[2022-06-06 09:47:44,599][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:47:44,618][fairseq.trainer][INFO] - begin training epoch 12
[2022-06-06 09:47:44,618][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:48:06,430][train_inner][INFO] - {"epoch": 12, "update": 11.083, "loss": "0.236", "ce_loss": "0.361", "scl_loss": "0.222", "accuracy": "59.962", "ppl": "1.18", "wps": "111.6", "ups": "3.5", "wpb": "31.9", "bsz": "31.9", "num_updates": "8800", "lr": "5.356e-06", "gnorm": "0.749", "train_wall": "30", "gb_free": "22.7", "wall": "1810"}
[2022-06-06 09:48:37,365][train_inner][INFO] - {"epoch": 12, "update": 11.335, "loss": "0.233", "ce_loss": "0.351", "scl_loss": "0.22", "accuracy": "60.641", "ppl": "1.18", "wps": "206.9", "ups": "6.47", "wpb": "32", "bsz": "32", "num_updates": "9000", "lr": "5.455e-06", "gnorm": "0.777", "train_wall": "31", "gb_free": "23", "wall": "1841"}
[2022-06-06 09:48:37,366][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[341 138 250  46]
 [ 85 188  79  39]
 [ 42  23 106  35]
 [  3  13  44  12]]
SENSE:  0.4573991030022724
SPEC:  0.44
[2022-06-06 09:48:51,091][valid][INFO] - {"epoch": 12, "valid_loss": "0.336", "valid_ce_loss": "0.86", "valid_scl_loss": "0.278", "valid_accuracy": "44.806", "valid_ppl": "1.26", "valid_wps": "755.3", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.4487", "valid_num_updates": "9000", "valid_best_icbhi": "0.524931"}
[2022-06-06 09:48:51,092][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 9000 updates
[2022-06-06 09:48:51,093][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_12_9000.pt
[2022-06-06 09:48:52,250][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_12_9000.pt
[2022-06-06 09:48:53,160][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_12_9000.pt (epoch 12 @ 9000 updates, score 0.44869955150113616) (writing took 2.0670292389113456 seconds)
[2022-06-06 09:49:22,483][train_inner][INFO] - {"epoch": 12, "update": 11.587, "loss": "0.232", "ce_loss": "0.343", "scl_loss": "0.22", "accuracy": "61.557", "ppl": "1.17", "wps": "141.5", "ups": "4.43", "wpb": "31.9", "bsz": "31.9", "num_updates": "9200", "lr": "5.554e-06", "gnorm": "0.834", "train_wall": "29", "gb_free": "23", "wall": "1886"}
[2022-06-06 09:49:52,466][train_inner][INFO] - {"epoch": 12, "update": 11.839, "loss": "0.232", "ce_loss": "0.345", "scl_loss": "0.219", "accuracy": "62.062", "ppl": "1.17", "wps": "213.5", "ups": "6.67", "wpb": "32", "bsz": "32", "num_updates": "9400", "lr": "5.653e-06", "gnorm": "0.796", "train_wall": "30", "gb_free": "23.1", "wall": "1916"}
[2022-06-06 09:50:11,748][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[341 138 250  46]
 [ 85 188  79  39]
 [ 42  23 106  35]
 [  3  13  44  12]]
SENSE:  0.4573991030022724
SPEC:  0.44
[775.0, 391.0, 206.0000001, 72.0000001]
[[508 135 117  15]
 [142 222  20   7]
 [ 95  41  55  15]
 [ 11  18  37   6]]
SENSE:  0.4230194318615787
SPEC:  0.655483870967742
[2022-06-06 09:50:25,589][valid][INFO] - {"epoch": 12, "valid_loss": "0.297", "valid_ce_loss": "0.682", "valid_scl_loss": "0.255", "valid_accuracy": "54.778", "valid_ppl": "1.23", "valid_wps": "739.4", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.539252", "valid_num_updates": "9528", "valid_best_icbhi": "0.539252"}
[2022-06-06 09:50:25,591][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 12 @ 9528 updates
[2022-06-06 09:50:25,592][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[508 135 117  15]
 [142 222  20   7]
 [ 95  41  55  15]
 [ 11  18  37   6]]
SENSE:  0.4230194318615787
SPEC:  0.655483870967742
[2022-06-06 09:50:26,742][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_best.pt
[2022-06-06 09:50:27,714][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_best.pt (epoch 12 @ 9528 updates, score 0.5392516514146604) (writing took 2.1228558480506763 seconds)
[2022-06-06 09:50:27,714][fairseq_cli.train][INFO] - end of epoch 12 (average epoch stats below)
[2022-06-06 09:50:27,716][train][INFO] - {"epoch": 12, "train_loss": "0.232", "train_ce_loss": "0.346", "train_scl_loss": "0.22", "train_accuracy": "61.482", "train_ppl": "1.17", "train_wps": "155.5", "train_ups": "4.87", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "9528", "train_lr": "5.71636e-06", "train_gnorm": "0.813", "train_train_wall": "118", "train_gb_free": "22.8", "train_wall": "1951"}
[2022-06-06 09:50:27,747][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:50:27,771][fairseq.trainer][INFO] - begin training epoch 13
[2022-06-06 09:50:27,771][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:50:50,389][train_inner][INFO] - {"epoch": 13, "update": 12.091, "loss": "0.232", "ce_loss": "0.343", "scl_loss": "0.219", "accuracy": "61.543", "ppl": "1.17", "wps": "110.1", "ups": "3.45", "wpb": "31.9", "bsz": "31.9", "num_updates": "9600", "lr": "5.752e-06", "gnorm": "0.916", "train_wall": "30", "gb_free": "22.6", "wall": "1974"}
[2022-06-06 09:51:19,540][train_inner][INFO] - {"epoch": 13, "update": 12.343, "loss": "0.23", "ce_loss": "0.341", "scl_loss": "0.218", "accuracy": "61.953", "ppl": "1.17", "wps": "219.6", "ups": "6.86", "wpb": "32", "bsz": "32", "num_updates": "9800", "lr": "5.851e-06", "gnorm": "0.924", "train_wall": "29", "gb_free": "23.3", "wall": "2003"}
[2022-06-06 09:51:50,262][train_inner][INFO] - {"epoch": 13, "update": 12.594, "loss": "0.23", "ce_loss": "0.335", "scl_loss": "0.218", "accuracy": "62.872", "ppl": "1.17", "wps": "207.6", "ups": "6.51", "wpb": "31.9", "bsz": "31.9", "num_updates": "10000", "lr": "5.95e-06", "gnorm": "0.914", "train_wall": "30", "gb_free": "22.6", "wall": "2033"}
[2022-06-06 09:51:50,263][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[475 172  94  34]
 [121 235  17  18]
 [ 84  38  48  36]
 [  8  19  37   8]]
SENSE:  0.4349775783452982
SPEC:  0.6129032258064516
[2022-06-06 09:52:04,006][valid][INFO] - {"epoch": 13, "valid_loss": "0.302", "valid_ce_loss": "0.691", "valid_scl_loss": "0.259", "valid_accuracy": "53.047", "valid_ppl": "1.23", "valid_wps": "752", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.52394", "valid_num_updates": "10000", "valid_best_icbhi": "0.539252"}
[2022-06-06 09:52:04,008][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 10000 updates
[2022-06-06 09:52:04,009][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_13_10000.pt
[2022-06-06 09:52:05,383][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_13_10000.pt
[2022-06-06 09:52:06,505][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_13_10000.pt (epoch 13 @ 10000 updates, score 0.5239404020758749) (writing took 2.4970833871047944 seconds)
[2022-06-06 09:52:36,735][train_inner][INFO] - {"epoch": 13, "update": 12.846, "loss": "0.231", "ce_loss": "0.341", "scl_loss": "0.219", "accuracy": "62.219", "ppl": "1.17", "wps": "137.7", "ups": "4.3", "wpb": "32", "bsz": "32", "num_updates": "10200", "lr": "6.049e-06", "gnorm": "0.978", "train_wall": "30", "gb_free": "23.2", "wall": "2080"}
[2022-06-06 09:52:55,434][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[475 172  94  34]
 [121 235  17  18]
 [ 84  38  48  36]
 [  8  19  37   8]]
SENSE:  0.4349775783452982
SPEC:  0.6129032258064516
[775.0, 391.0, 206.0000001, 72.0000001]
[[396 180 153  46]
 [107 245  24  15]
 [ 67  46  59  34]
 [  7  19  37   9]]
SENSE:  0.467862481175527
SPEC:  0.5109677419354839
[2022-06-06 09:53:09,329][valid][INFO] - {"epoch": 13, "valid_loss": "0.317", "valid_ce_loss": "0.793", "valid_scl_loss": "0.264", "valid_accuracy": "49.1", "valid_ppl": "1.25", "valid_wps": "746.9", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.489415", "valid_num_updates": "10322", "valid_best_icbhi": "0.539252"}
[2022-06-06 09:53:09,330][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 13 @ 10322 updates
[2022-06-06 09:53:09,331][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[396 180 153  46]
 [107 245  24  15]
 [ 67  46  59  34]
 [  7  19  37   9]]
SENSE:  0.467862481175527
SPEC:  0.5109677419354839
[2022-06-06 09:53:10,742][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:53:10,753][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 13 @ 10322 updates, score 0.48941511155550543) (writing took 1.4221168020740151 seconds)
[2022-06-06 09:53:10,753][fairseq_cli.train][INFO] - end of epoch 13 (average epoch stats below)
[2022-06-06 09:53:10,755][train][INFO] - {"epoch": 13, "train_loss": "0.23", "train_ce_loss": "0.337", "train_scl_loss": "0.218", "train_accuracy": "62.475", "train_ppl": "1.17", "train_wps": "155.6", "train_ups": "4.87", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "10322", "train_lr": "6.10939e-06", "train_gnorm": "0.941", "train_train_wall": "119", "train_gb_free": "22.8", "train_wall": "2114"}
[2022-06-06 09:53:10,791][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:53:10,823][fairseq.trainer][INFO] - begin training epoch 14
[2022-06-06 09:53:10,823][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:53:34,338][train_inner][INFO] - {"epoch": 14, "update": 13.098, "loss": "0.226", "ce_loss": "0.325", "scl_loss": "0.216", "accuracy": "64.207", "ppl": "1.17", "wps": "110.8", "ups": "3.47", "wpb": "31.9", "bsz": "31.9", "num_updates": "10400", "lr": "6.148e-06", "gnorm": "0.979", "train_wall": "29", "gb_free": "23.2", "wall": "2138"}
[2022-06-06 09:54:04,582][train_inner][INFO] - {"epoch": 14, "update": 13.35, "loss": "0.229", "ce_loss": "0.334", "scl_loss": "0.217", "accuracy": "63.75", "ppl": "1.17", "wps": "211.6", "ups": "6.61", "wpb": "32", "bsz": "32", "num_updates": "10600", "lr": "6.247e-06", "gnorm": "0.992", "train_wall": "30", "gb_free": "22.8", "wall": "2168"}
[2022-06-06 09:54:34,726][train_inner][INFO] - {"epoch": 14, "update": 13.602, "loss": "0.224", "ce_loss": "0.314", "scl_loss": "0.214", "accuracy": "65.128", "ppl": "1.17", "wps": "212.1", "ups": "6.64", "wpb": "32", "bsz": "32", "num_updates": "10800", "lr": "6.346e-06", "gnorm": "1.065", "train_wall": "30", "gb_free": "22.7", "wall": "2198"}
[2022-06-06 09:55:04,923][train_inner][INFO] - {"epoch": 14, "update": 13.854, "loss": "0.223", "ce_loss": "0.314", "scl_loss": "0.213", "accuracy": "65.219", "ppl": "1.17", "wps": "212", "ups": "6.62", "wpb": "32", "bsz": "32", "num_updates": "11000", "lr": "6.445e-06", "gnorm": "1.058", "train_wall": "30", "gb_free": "22.5", "wall": "2228"}
[2022-06-06 09:55:04,924][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[317 292 125  41]
 [ 59 297  19  16]
 [ 50  73  47  36]
 [  6  21  33  12]]
SENSE:  0.5321375185255195
SPEC:  0.40903225806451615
[2022-06-06 09:55:18,828][valid][INFO] - {"epoch": 14, "valid_loss": "0.334", "valid_ce_loss": "0.928", "valid_scl_loss": "0.268", "valid_accuracy": "46.607", "valid_ppl": "1.26", "valid_wps": "746.9", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.470585", "valid_num_updates": "11000", "valid_best_icbhi": "0.539252"}
[2022-06-06 09:55:18,830][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 11000 updates
[2022-06-06 09:55:18,831][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_14_11000.pt
[2022-06-06 09:55:20,203][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_14_11000.pt
[2022-06-06 09:55:21,335][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_14_11000.pt (epoch 14 @ 11000 updates, score 0.4705848882950178) (writing took 2.5050568730803207 seconds)
[2022-06-06 09:55:38,972][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[317 292 125  41]
 [ 59 297  19  16]
 [ 50  73  47  36]
 [  6  21  33  12]]
SENSE:  0.5321375185255195
SPEC:  0.40903225806451615
[775.0, 391.0, 206.0000001, 72.0000001]
[[392 252  99  32]
 [ 82 277  21  11]
 [ 67  56  51  32]
 [  7  21  35   9]]
SENSE:  0.5037369206266856
SPEC:  0.5058064516129033
[2022-06-06 09:55:52,694][valid][INFO] - {"epoch": 14, "valid_loss": "0.314", "valid_ce_loss": "0.805", "valid_scl_loss": "0.259", "valid_accuracy": "50.485", "valid_ppl": "1.24", "valid_wps": "750", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.504772", "valid_num_updates": "11116", "valid_best_icbhi": "0.539252"}
[2022-06-06 09:55:52,695][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 14 @ 11116 updates
[2022-06-06 09:55:52,696][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[392 252  99  32]
 [ 82 277  21  11]
 [ 67  56  51  32]
 [  7  21  35   9]]
SENSE:  0.5037369206266856
SPEC:  0.5058064516129033
[2022-06-06 09:55:53,825][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:55:53,834][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 14 @ 11116 updates, score 0.5047716861197944) (writing took 1.1388961849734187 seconds)
[2022-06-06 09:55:53,835][fairseq_cli.train][INFO] - end of epoch 14 (average epoch stats below)
[2022-06-06 09:55:53,837][train][INFO] - {"epoch": 14, "train_loss": "0.225", "train_ce_loss": "0.32", "train_scl_loss": "0.215", "train_accuracy": "64.939", "train_ppl": "1.17", "train_wps": "155.6", "train_ups": "4.87", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "11116", "train_lr": "6.50242e-06", "train_gnorm": "1.052", "train_train_wall": "118", "train_gb_free": "23.2", "train_wall": "2277"}
[2022-06-06 09:55:53,868][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:55:53,893][fairseq.trainer][INFO] - begin training epoch 15
[2022-06-06 09:55:53,893][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:56:19,115][train_inner][INFO] - {"epoch": 15, "update": 14.106, "loss": "0.222", "ce_loss": "0.304", "scl_loss": "0.213", "accuracy": "66.954", "ppl": "1.17", "wps": "85.9", "ups": "2.7", "wpb": "31.9", "bsz": "31.9", "num_updates": "11200", "lr": "6.544e-06", "gnorm": "1.118", "train_wall": "30", "gb_free": "23.1", "wall": "2302"}
[2022-06-06 09:56:49,420][train_inner][INFO] - {"epoch": 15, "update": 14.358, "loss": "0.222", "ce_loss": "0.309", "scl_loss": "0.212", "accuracy": "66.562", "ppl": "1.17", "wps": "211.2", "ups": "6.6", "wpb": "32", "bsz": "32", "num_updates": "11400", "lr": "6.643e-06", "gnorm": "1.171", "train_wall": "30", "gb_free": "23", "wall": "2333"}
[2022-06-06 09:57:19,521][train_inner][INFO] - {"epoch": 15, "update": 14.61, "loss": "0.224", "ce_loss": "0.316", "scl_loss": "0.214", "accuracy": "65.675", "ppl": "1.17", "wps": "212.2", "ups": "6.65", "wpb": "31.9", "bsz": "31.9", "num_updates": "11600", "lr": "6.742e-06", "gnorm": "1.259", "train_wall": "30", "gb_free": "23.1", "wall": "2363"}
[2022-06-06 09:57:49,503][train_inner][INFO] - {"epoch": 15, "update": 14.861, "loss": "0.221", "ce_loss": "0.312", "scl_loss": "0.211", "accuracy": "65.734", "ppl": "1.17", "wps": "213.5", "ups": "6.67", "wpb": "32", "bsz": "32", "num_updates": "11800", "lr": "6.841e-06", "gnorm": "1.273", "train_wall": "30", "gb_free": "23.1", "wall": "2393"}
[2022-06-06 09:58:06,208][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[371 228 113  63]
 [ 82 261  23  25]
 [ 68  50  53  35]
 [  7  20  35  10]]
SENSE:  0.48430493259064133
SPEC:  0.47870967741935483
[2022-06-06 09:58:20,085][valid][INFO] - {"epoch": 15, "valid_loss": "0.321", "valid_ce_loss": "0.853", "valid_scl_loss": "0.262", "valid_accuracy": "48.13", "valid_ppl": "1.25", "valid_wps": "748.9", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.481507", "valid_num_updates": "11910", "valid_best_icbhi": "0.539252"}
[2022-06-06 09:58:20,086][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 15 @ 11910 updates
[2022-06-06 09:58:20,087][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[371 228 113  63]
 [ 82 261  23  25]
 [ 68  50  53  35]
 [  7  20  35  10]]
SENSE:  0.48430493259064133
SPEC:  0.47870967741935483
[2022-06-06 09:58:21,459][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 09:58:21,467][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 15 @ 11910 updates, score 0.48150730500499805) (writing took 1.380887100007385 seconds)
[2022-06-06 09:58:21,468][fairseq_cli.train][INFO] - end of epoch 15 (average epoch stats below)
[2022-06-06 09:58:21,469][train][INFO] - {"epoch": 15, "train_loss": "0.222", "train_ce_loss": "0.311", "train_scl_loss": "0.213", "train_accuracy": "66.094", "train_ppl": "1.17", "train_wps": "171.8", "train_ups": "5.38", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "11910", "train_lr": "6.89545e-06", "train_gnorm": "1.227", "train_train_wall": "118", "train_gb_free": "23", "train_wall": "2425"}
[2022-06-06 09:58:21,500][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 09:58:21,526][fairseq.trainer][INFO] - begin training epoch 16
[2022-06-06 09:58:21,527][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 09:58:46,680][train_inner][INFO] - {"epoch": 16, "update": 15.113, "loss": "0.221", "ce_loss": "0.305", "scl_loss": "0.211", "accuracy": "67.337", "ppl": "1.17", "wps": "111.3", "ups": "3.5", "wpb": "31.8", "bsz": "31.8", "num_updates": "12000", "lr": "6.94e-06", "gnorm": "1.313", "train_wall": "30", "gb_free": "22.8", "wall": "2450"}
[2022-06-06 09:58:46,681][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[432 167 147  29]
 [116 235  29  11]
 [ 72  42  71  21]
 [  6  19  40   7]]
SENSE:  0.467862481175527
SPEC:  0.5574193548387096
[2022-06-06 09:59:00,465][valid][INFO] - {"epoch": 16, "valid_loss": "0.313", "valid_ce_loss": "0.799", "valid_scl_loss": "0.259", "valid_accuracy": "51.593", "valid_ppl": "1.24", "valid_wps": "750.6", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.512641", "valid_num_updates": "12000", "valid_best_icbhi": "0.539252"}
[2022-06-06 09:59:00,466][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 12000 updates
[2022-06-06 09:59:00,467][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_16_12000.pt
[2022-06-06 09:59:01,824][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_16_12000.pt
[2022-06-06 09:59:02,910][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_16_12000.pt (epoch 16 @ 12000 updates, score 0.5126409180071183) (writing took 2.443252471042797 seconds)
[2022-06-06 09:59:33,714][train_inner][INFO] - {"epoch": 16, "update": 15.365, "loss": "0.219", "ce_loss": "0.296", "scl_loss": "0.21", "accuracy": "67.953", "ppl": "1.16", "wps": "136.1", "ups": "4.25", "wpb": "32", "bsz": "32", "num_updates": "12200", "lr": "7.039e-06", "gnorm": "1.352", "train_wall": "31", "gb_free": "22.8", "wall": "2497"}
[2022-06-06 10:00:03,744][train_inner][INFO] - {"epoch": 16, "update": 15.617, "loss": "0.218", "ce_loss": "0.289", "scl_loss": "0.211", "accuracy": "68.469", "ppl": "1.16", "wps": "213.1", "ups": "6.66", "wpb": "32", "bsz": "32", "num_updates": "12400", "lr": "7.138e-06", "gnorm": "1.325", "train_wall": "30", "gb_free": "22.7", "wall": "2527"}
[2022-06-06 10:00:33,144][train_inner][INFO] - {"epoch": 16, "update": 15.869, "loss": "0.216", "ce_loss": "0.29", "scl_loss": "0.208", "accuracy": "68.469", "ppl": "1.16", "wps": "217.7", "ups": "6.8", "wpb": "32", "bsz": "32", "num_updates": "12600", "lr": "7.237e-06", "gnorm": "1.411", "train_wall": "29", "gb_free": "23", "wall": "2556"}
[2022-06-06 10:00:48,749][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[432 167 147  29]
 [116 235  29  11]
 [ 72  42  71  21]
 [  6  19  40   7]]
SENSE:  0.467862481175527
SPEC:  0.5574193548387096
[775.0, 391.0, 206.0000001, 72.0000001]
[[254 114 321  86]
 [ 50 192  74  75]
 [ 25  16 116  49]
 [  5   8  41  18]]
SENSE:  0.4872944692115712
SPEC:  0.327741935483871
[2022-06-06 10:01:02,579][valid][INFO] - {"epoch": 16, "valid_loss": "0.365", "valid_ce_loss": "1.206", "valid_scl_loss": "0.271", "valid_accuracy": "40.166", "valid_ppl": "1.29", "valid_wps": "744", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.407518", "valid_num_updates": "12704", "valid_best_icbhi": "0.539252"}
[2022-06-06 10:01:02,581][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 16 @ 12704 updates
[2022-06-06 10:01:02,582][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[254 114 321  86]
 [ 50 192  74  75]
 [ 25  16 116  49]
 [  5   8  41  18]]
SENSE:  0.4872944692115712
SPEC:  0.327741935483871
[2022-06-06 10:01:03,944][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 10:01:03,954][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 16 @ 12704 updates, score 0.40751820234772107) (writing took 1.3731043200241402 seconds)
[2022-06-06 10:01:03,955][fairseq_cli.train][INFO] - end of epoch 16 (average epoch stats below)
[2022-06-06 10:01:03,958][train][INFO] - {"epoch": 16, "train_loss": "0.218", "train_ce_loss": "0.295", "train_scl_loss": "0.21", "train_accuracy": "68.175", "train_ppl": "1.16", "train_wps": "156.1", "train_ups": "4.89", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "12704", "train_lr": "7.28848e-06", "train_gnorm": "1.389", "train_train_wall": "118", "train_gb_free": "22.9", "train_wall": "2587"}
[2022-06-06 10:01:03,994][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 10:01:04,022][fairseq.trainer][INFO] - begin training epoch 17
[2022-06-06 10:01:04,022][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 10:01:30,621][train_inner][INFO] - {"epoch": 17, "update": 16.121, "loss": "0.219", "ce_loss": "0.301", "scl_loss": "0.21", "accuracy": "68.491", "ppl": "1.16", "wps": "110.9", "ups": "3.48", "wpb": "31.9", "bsz": "31.9", "num_updates": "12800", "lr": "7.336e-06", "gnorm": "1.473", "train_wall": "30", "gb_free": "23.1", "wall": "2614"}
[2022-06-06 10:02:00,766][train_inner][INFO] - {"epoch": 17, "update": 16.373, "loss": "0.216", "ce_loss": "0.296", "scl_loss": "0.207", "accuracy": "68.071", "ppl": "1.16", "wps": "211.9", "ups": "6.64", "wpb": "31.9", "bsz": "31.9", "num_updates": "13000", "lr": "7.435e-06", "gnorm": "1.602", "train_wall": "30", "gb_free": "23", "wall": "2644"}
[2022-06-06 10:02:00,767][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[294  85 231 165]
 [ 67 128  52 144]
 [ 28   5  81  92]
 [  4   4  33  31]]
SENSE:  0.3587443945115862
SPEC:  0.3793548387096774
[2022-06-06 10:02:14,252][valid][INFO] - {"epoch": 17, "valid_loss": "0.377", "valid_ce_loss": "1.279", "valid_scl_loss": "0.277", "valid_accuracy": "36.981", "valid_ppl": "1.3", "valid_wps": "751.2", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.36905", "valid_num_updates": "13000", "valid_best_icbhi": "0.539252"}
[2022-06-06 10:02:14,254][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 13000 updates
[2022-06-06 10:02:14,257][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_17_13000.pt
[2022-06-06 10:02:15,584][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_17_13000.pt
[2022-06-06 10:02:16,716][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_17_13000.pt (epoch 17 @ 13000 updates, score 0.3690496166106318) (writing took 2.4623291270108894 seconds)
[2022-06-06 10:02:46,998][train_inner][INFO] - {"epoch": 17, "update": 16.625, "loss": "0.216", "ce_loss": "0.283", "scl_loss": "0.208", "accuracy": "69.594", "ppl": "1.16", "wps": "138.4", "ups": "4.33", "wpb": "32", "bsz": "32", "num_updates": "13200", "lr": "7.534e-06", "gnorm": "1.42", "train_wall": "30", "gb_free": "23.1", "wall": "2690"}
[2022-06-06 10:03:17,158][train_inner][INFO] - {"epoch": 17, "update": 16.877, "loss": "0.216", "ce_loss": "0.291", "scl_loss": "0.208", "accuracy": "68.547", "ppl": "1.16", "wps": "212.2", "ups": "6.63", "wpb": "32", "bsz": "32", "num_updates": "13400", "lr": "7.633e-06", "gnorm": "1.516", "train_wall": "30", "gb_free": "22.8", "wall": "2720"}
[2022-06-06 10:03:31,456][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[294  85 231 165]
 [ 67 128  52 144]
 [ 28   5  81  92]
 [  4   4  33  31]]
SENSE:  0.3587443945115862
SPEC:  0.3793548387096774
[775.0, 391.0, 206.0000001, 72.0000001]
[[324 236 152  63]
 [ 64 273  25  29]
 [ 50  55  65  36]
 [  4  19  37  12]]
SENSE:  0.5231689086627298
SPEC:  0.4180645161290323
[2022-06-06 10:03:45,186][valid][INFO] - {"epoch": 17, "valid_loss": "0.33", "valid_ce_loss": "0.931", "valid_scl_loss": "0.263", "valid_accuracy": "46.676", "valid_ppl": "1.26", "valid_wps": "759.3", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.470617", "valid_num_updates": "13498", "valid_best_icbhi": "0.539252"}
[2022-06-06 10:03:45,188][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 17 @ 13498 updates
[2022-06-06 10:03:45,189][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[775.0, 391.0, 206.0000001, 72.0000001]
[[324 236 152  63]
 [ 64 273  25  29]
 [ 50  55  65  36]
 [  4  19  37  12]]
SENSE:  0.5231689086627298
SPEC:  0.4180645161290323
[2022-06-06 10:03:46,293][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_last.pt
[2022-06-06 10:03:46,300][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_last.pt (epoch 17 @ 13498 updates, score 0.47061671239588104) (writing took 1.1122274160152301 seconds)
[2022-06-06 10:03:46,301][fairseq_cli.train][INFO] - end of epoch 17 (average epoch stats below)
[2022-06-06 10:03:46,302][train][INFO] - {"epoch": 17, "train_loss": "0.216", "train_ce_loss": "0.289", "train_scl_loss": "0.208", "train_accuracy": "69.015", "train_ppl": "1.16", "train_wps": "156.3", "train_ups": "4.89", "train_wpb": "32", "train_bsz": "32", "train_num_updates": "13498", "train_lr": "7.68151e-06", "train_gnorm": "1.492", "train_train_wall": "118", "train_gb_free": "23.2", "train_wall": "2749"}
[2022-06-06 10:03:46,332][fairseq.data.iterators][INFO] - grouped total_num_itrs = 794
[2022-06-06 10:03:46,356][fairseq.trainer][INFO] - begin training epoch 18
[2022-06-06 10:03:46,356][fairseq_cli.train][INFO] - Start iterating over samples
[2022-06-06 10:04:14,134][train_inner][INFO] - {"epoch": 18, "update": 17.128, "loss": "0.213", "ce_loss": "0.274", "scl_loss": "0.206", "accuracy": "70.638", "ppl": "1.16", "wps": "111.7", "ups": "3.51", "wpb": "31.8", "bsz": "31.8", "num_updates": "13600", "lr": "7.732e-06", "gnorm": "1.481", "train_wall": "29", "gb_free": "23", "wall": "2777"}
[2022-06-06 10:04:43,881][train_inner][INFO] - {"epoch": 18, "update": 17.38, "loss": "0.211", "ce_loss": "0.26", "scl_loss": "0.205", "accuracy": "72.188", "ppl": "1.16", "wps": "215.2", "ups": "6.72", "wpb": "32", "bsz": "32", "num_updates": "13800", "lr": "7.831e-06", "gnorm": "1.543", "train_wall": "29", "gb_free": "22.9", "wall": "2807"}
[2022-06-06 10:05:14,006][train_inner][INFO] - {"epoch": 18, "update": 17.632, "loss": "0.213", "ce_loss": "0.28", "scl_loss": "0.205", "accuracy": "70.213", "ppl": "1.16", "wps": "212.2", "ups": "6.64", "wpb": "32", "bsz": "32", "num_updates": "14000", "lr": "7.93e-06", "gnorm": "1.684", "train_wall": "30", "gb_free": "23.1", "wall": "2837"}
[2022-06-06 10:05:14,007][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[381 167 151  76]
 [101 217  24  49]
 [ 64  30  71  41]
 [  6  15  35  16]]
SENSE:  0.4544095663813425
SPEC:  0.4916129032258065
[2022-06-06 10:05:28,121][valid][INFO] - {"epoch": 18, "valid_loss": "0.339", "valid_ce_loss": "0.948", "valid_scl_loss": "0.271", "valid_accuracy": "47.438", "valid_ppl": "1.26", "valid_wps": "754.3", "valid_wpb": "31.4", "valid_bsz": "31.4", "valid_icbhi": "0.473011", "valid_num_updates": "14000", "valid_best_icbhi": "0.539252"}
[2022-06-06 10:05:28,122][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 18 @ 14000 updates
[2022-06-06 10:05:28,123][fairseq.trainer][INFO] - Saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_18_14000.pt
[2022-06-06 10:05:29,217][fairseq.trainer][INFO] - Finished saving checkpoint to /media/SSD/tungtk2/fairseq/outputs/2022-06-06/09-17-46/checkpoints/checkpoint_18_14000.pt
[2022-06-06 10:05:30,081][fairseq.checkpoint_utils][INFO] - Saved checkpoint checkpoints/checkpoint_18_14000.pt (epoch 18 @ 14000 updates, score 0.4730112348035745) (writing took 1.9584652080666274 seconds)
[2022-06-06 10:06:00,317][train_inner][INFO] - {"epoch": 18, "update": 17.884, "loss": "0.209", "ce_loss": "0.269", "scl_loss": "0.202", "accuracy": "71.078", "ppl": "1.16", "wps": "138.2", "ups": "4.32", "wpb": "32", "bsz": "32", "num_updates": "14200", "lr": "8.029e-06", "gnorm": "1.641", "train_wall": "30", "gb_free": "22.9", "wall": "2883"}
[2022-06-06 10:06:13,971][fairseq_cli.train][INFO] - begin validation on "valid" subset
[775.0, 391.0, 206.0000001, 72.0000001]
[[381 167 151  76]
 [101 217  24  49]
 [ 64  30  71  41]
 [  6  15  35  16]]
SENSE:  0.4544095663813425
SPEC:  0.4916129032258065
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-2ntknau9'
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-mx5jbe8c'
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-07jtw3c_'
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-egrl_0j3'
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-spha6szh'
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-vkjd675y'
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-f_m4k86j'
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/queues.py", line 239, in _feed
    obj = _ForkingPickler.dumps(obj)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/media/SSD/tungtk2/fairseq_new_env/lib/python3.8/site-packages/torch/multiprocessing/reductions.py", line 348, in reduce_storage
    df = multiprocessing.reduction.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/reduction.py", line 198, in DupFd
    return resource_sharer.DupFd(fd)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 53, in __init__
    self._id = _resource_sharer.register(send, close)
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 77, in register
    self._start()
  File "/usr/lib/python3.8/multiprocessing/resource_sharer.py", line 130, in _start
    self._listener = Listener(authkey=process.current_process().authkey)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 442, in __init__
    address = address or arbitrary_address(family)
  File "/usr/lib/python3.8/multiprocessing/connection.py", line 76, in arbitrary_address
    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())
  File "/usr/lib/python3.8/multiprocessing/util.py", line 146, in get_temp_dir
    tempdir = tempfile.mkdtemp(prefix='pymp-')
  File "/usr/lib/python3.8/tempfile.py", line 497, in mkdtemp
    _os.mkdir(file, 0o700)
OSError: [Errno 28] No space left on device: '/tmp/pymp-h1k1iebk'
/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 662 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
